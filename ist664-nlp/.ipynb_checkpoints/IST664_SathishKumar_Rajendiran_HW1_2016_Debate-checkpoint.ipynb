{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sathish Kumar Rajendiran\n",
    "Homework: 1 | debate 2016 Trump vs Hillary\n",
    "Date: 10/18/2020\n",
    "Subject: 2020-0930 IST 664- Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sathishrajendiran/ist664-nlp'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# csv, xls, pdf, pandas & json\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import xlrd\n",
    "\n",
    "# Language Processing\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "##  Regular Expression to match non-alphabetic characters\n",
    "import re\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if(pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile(' \\w+(?:-\\w+)*') # words with internal hyphens or apostrophes\n",
    "    if(pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile('[''\\\".?!,:;/]+') # words with internal hyphens or apostrophes\n",
    "    if(pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw text is: 92599\n",
      "Displaying 25 of 27 matches:\n",
      "andidates : Democratic nominee for president of the United States , Hillary Cli\n",
      "inton , and Republican nominee for president of the United States , Donald J. T\n",
      "at in my book ... TRUMP : So is it President Obama 's fault ? [ Interruption ] \n",
      "you even announced . TRUMP : Is it President Obama 's fault ? [ Interruption ] \n",
      "nces ... TRUMP : Secretary , is it President Obama 's fault ? [ Interruption ] \n",
      "her . Because we have -- we have a president that ca n't sit them around a tabl\n",
      "oters will know if their potential president owes money to -- who he owes it to\n",
      "or 40 years , everyone running for president has released their tax returns . Y\n",
      "f your -- if your main claim to be president of the United States is your busin\n",
      "n killed since Barack Obama became president . Over 4 -- almost 4,000 people in\n",
      " I prepared for ? I prepared to be president . And I think that 's a good thing\n",
      "claim of the nation 's first black president was not a natural born citizen . Y\n",
      "cans have accepted for years , the president was born in the United States . Ca\n",
      "he campaign , her campaign against President Obama , fought very hard , and you\n",
      "to tell the story and question the president 's legitimacy in 2012 , '13 , '14 \n",
      "y for the country but even for the president in getting him to produce his birt\n",
      "is racist lie that our first black president was not an American citizen . Ther\n",
      "biggest challenges facing the next president , because clearly we 're facing at\n",
      "now , because the truth is , under President Obama we 've lost control of thing\n",
      " -- we will take out ISIS . Well , President Obama and Secretary Clinton create\n",
      " And my successor , John Kerry and President Obama got a deal that put a lid on\n",
      " warming , like you think and your president thinks . Nuclear is the single gre\n",
      "ing America . On nuclear weapons , President Obama reportedly considered changi\n",
      "er . Words matter when you run for president , and they really matter when you \n",
      "nd they really matter when you are president . And I want to reassure our allie\n"
     ]
    }
   ],
   "source": [
    "# Working with file\n",
    "try: \n",
    "    debatefile = open('/Users/sathishrajendiran/ist664-nlp/presidential_debate_2016.txt', 'r')\n",
    "    rawText = debatefile.read()\n",
    "    print('length of raw text is:',len(rawText))\n",
    "    \n",
    "except:\n",
    "    print(\"Is the file in correct directory?\")\n",
    "    \n",
    "debateTokens = nltk.word_tokenize(rawText)\n",
    "debateText = nltk.Text(debateTokens)\n",
    "debateText.concordance('president')\n",
    "#When we are done, we close the file.\n",
    "debatefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose to treat upper and lower case the same\n",
    "#    by putting all tokens in lower case\n",
    "# convert to lower case\n",
    "debateTokens = [w.lower() for w in debateTokens]\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in debateTokens]\n",
    "filewords = [w for w in stripped if w.isalpha() and len(debateTokens)>2]\n",
    "\n",
    "filewords = [w.lower() for w in debateTokens]\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filewords = [w for w in debateTokens if not w in stop_words] \n",
    "\n",
    "custom_stopwords =['ơ̹','ơ','ơͣ','ơǝ','ơǝǝ','ơǝ','ơ','ơͥ','ǝǝ','Ơ̜','Ơ̜','ve','gon','na','you'\n",
    "                   ,'they','we','\\'re','\\'s','n\\'t','\\'ve','ơǝ']\n",
    "filewords = [w for w in debateTokens if not w in custom_stopwords] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 100 words from file:\n",
      "['full', 'transcript', ':', 'first', '2016', 'presidential', 'debate', 'by', 'politico', 'staff', '09/27/2016', '01:55', 'am', 'edt', 'holt', ':', 'good', 'evening', 'from', 'hofstra', 'university', 'in', 'hempstead', ',', 'new', 'york', '.', 'i', \"'m\", 'lester', 'holt', ',', 'anchor', 'of', '``', 'nbc', 'nightly', 'news', '.', \"''\", 'i', 'want', 'to', 'welcome', 'to', 'the', 'first', 'presidential', 'debate', '.', 'the', 'participants', 'tonight', 'are', 'donald', 'trump', 'and', 'hillary', 'clinton', '.', 'this', 'debate', 'is', 'sponsored', 'by', 'the', 'commission', 'on', 'presidential', 'debates', ',', 'a', 'nonpartisan', ',', 'nonprofit', 'organization', '.', 'the', 'commission', 'drafted', 'tonight', 'format', ',', 'and', 'the', 'rules', 'have', 'been', 'agreed', 'to', 'by', 'the', 'campaigns', '.', 'the', '90-minute', 'debate', 'is', 'divided', 'into', 'six', 'segments', ',', 'each', '15', 'minutes', 'long', '.', \"'ll\", 'explore', 'three', 'topic', 'areas', 'tonight', ':', 'achieving', 'prosperity', ';', 'america', 'direction', ';', 'and', 'securing', 'america', '.', 'at', 'the', 'start', 'of', 'each', 'segment', ',', 'i', 'will', 'ask', 'the', 'same', 'lead-off', 'question', 'to', 'both', 'candidates', ',', 'and', 'will', 'each', 'have', 'up', 'to', 'two', 'minutes', 'to', 'respond', '.', 'from', 'that', 'point', 'until', 'the', 'end', 'of', 'the', 'segment', ',', \"'ll\", 'have', 'an', 'open', 'discussion', '.', 'the', 'questions', 'are', 'mine', 'and', 'have', 'not', 'been', 'shared', 'with', 'the', 'commission', 'or', 'the', 'campaigns', '.', 'the', 'audience', 'here', 'in', 'the', 'room', 'has', 'agreed', 'to', 'remain', 'silent', 'so', 'that', 'can']\n"
     ]
    }
   ],
   "source": [
    "# display the first words\n",
    "print (\"Display first 100 words from file:\")\n",
    "print (filewords[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 100 Stopwords:\n",
      "['’', 's', 'a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'can', 'not', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning']\n"
     ]
    }
   ],
   "source": [
    "# read a stop word file\n",
    "fstop = open('Smart.English.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()\n",
    "\n",
    "stopwords = nltk.word_tokenize(stoptext)\n",
    "print (\"Display first 100 Stopwords:\")\n",
    "print (stopwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(filewords)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "finder.apply_word_filter(words_filter)\n",
    "finder.apply_word_filter(special_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 100 frequencies\n",
      "(('secretary', 'clinton'), 0.002107481559536354)\n",
      "(('mr.', 'trump'), 0.0012118018967334037)\n",
      "(('united', 'states'), 0.0006322444678609062)\n",
      "(('tax', 'returns'), 0.0005268703898840885)\n",
      "(('trade', 'deals'), 0.0005268703898840885)\n",
      "(('bring', 'back'), 0.0004741833508956797)\n",
      "(('middle', 'east'), 0.00042149631190727084)\n",
      "(('president', 'obama'), 0.00042149631190727084)\n",
      "(('barack', 'obama'), 0.00036880927291886193)\n",
      "(('long', 'time'), 0.00036880927291886193)\n",
      "(('sean', 'hannity'), 0.00036880927291886193)\n",
      "(('middle', 'class'), 0.0003161222339304531)\n",
      "(('birth', 'certificate'), 0.00026343519494204424)\n",
      "(('ca', 'bring'), 0.00026343519494204424)\n",
      "(('criminal', 'justice'), 0.00026343519494204424)\n",
      "(('north', 'korea'), 0.00026343519494204424)\n",
      "(('nuclear', 'weapons'), 0.00026343519494204424)\n",
      "(('achieving', 'prosperity'), 0.00021074815595363542)\n",
      "(('american', 'people'), 0.00021074815595363542)\n",
      "(('back', 'jobs'), 0.00021074815595363542)\n",
      "(('bad', 'experience'), 0.00021074815595363542)\n",
      "(('campaign', 'manager'), 0.00021074815595363542)\n",
      "(('defeat', 'isis'), 0.00021074815595363542)\n",
      "(('donald', 'trump'), 0.00021074815595363542)\n",
      "(('fair', 'share'), 0.00021074815595363542)\n",
      "(('final', 'question'), 0.00021074815595363542)\n",
      "(('iran', 'deal'), 0.00021074815595363542)\n",
      "(('justice', 'system'), 0.00021074815595363542)\n",
      "(('million', 'jobs'), 0.00021074815595363542)\n",
      "(('money', 'back'), 0.00021074815595363542)\n",
      "(('saudi', 'arabia'), 0.00021074815595363542)\n",
      "(('tax', 'cuts'), 0.00021074815595363542)\n",
      "(('african-american', 'community'), 0.00015806111696522654)\n",
      "(('bad', 'things'), 0.00015806111696522654)\n",
      "(('biggest', 'tax'), 0.00015806111696522654)\n",
      "(('fight', 'isis'), 0.00015806111696522654)\n",
      "(('good', 'job'), 0.00015806111696522654)\n",
      "(('great', 'thing'), 0.00015806111696522654)\n",
      "(('implicit', 'bias'), 0.00015806111696522654)\n",
      "(('interest', 'rates'), 0.00015806111696522654)\n",
      "(('million', 'people'), 0.00015806111696522654)\n",
      "(('obama', 'fault'), 0.00015806111696522654)\n",
      "(('police', 'officers'), 0.00015806111696522654)\n",
      "(('rising', 'incomes'), 0.00015806111696522654)\n",
      "(('securing', 'america'), 0.00015806111696522654)\n",
      "(('single', 'greatest'), 0.00015806111696522654)\n",
      "(('south', 'korea'), 0.00015806111696522654)\n",
      "(('wall', 'street'), 0.00015806111696522654)\n",
      "(('war', 'started'), 0.00015806111696522654)\n",
      "(('wealthy', 'people'), 0.00015806111696522654)\n",
      "(('years', 'ago'), 0.00015806111696522654)\n",
      "(('young', 'african-american'), 0.00015806111696522654)\n",
      "(('21st', 'century'), 0.00010537407797681771)\n",
      "(('america', 'direction'), 0.00010537407797681771)\n",
      "(('america', 'great'), 0.00010537407797681771)\n",
      "(('american', 'citizens'), 0.00010537407797681771)\n",
      "(('american', 'manufacturers'), 0.00010537407797681771)\n",
      "(('american', 'troops'), 0.00010537407797681771)\n",
      "(('american', 'workers'), 0.00010537407797681771)\n",
      "(('audit', 'complete'), 0.00010537407797681771)\n",
      "(('back', 'law'), 0.00010537407797681771)\n",
      "(('bad', 'people'), 0.00010537407797681771)\n",
      "(('beautiful', 'thing'), 0.00010537407797681771)\n",
      "(('bernie', 'sanders'), 0.00010537407797681771)\n",
      "(('big', 'league'), 0.00010537407797681771)\n",
      "(('birther', 'lie'), 0.00010537407797681771)\n",
      "(('black', 'president'), 0.00010537407797681771)\n",
      "(('bureaucratic', 'red'), 0.00010537407797681771)\n",
      "(('child', 'care'), 0.00010537407797681771)\n",
      "(('clean', 'energy'), 0.00010537407797681771)\n",
      "(('clinton', 'talks'), 0.00010537407797681771)\n",
      "(('common', 'sense'), 0.00010537407797681771)\n",
      "(('create', 'tremendous'), 0.00010537407797681771)\n",
      "(('current', 'mayor'), 0.00010537407797681771)\n",
      "(('cut', 'regulations'), 0.00010537407797681771)\n",
      "(('debtor', 'nation'), 0.00010537407797681771)\n",
      "(('defend', 'japan'), 0.00010537407797681771)\n",
      "(('democratic', 'national'), 0.00010537407797681771)\n",
      "(('equal', 'pay'), 0.00010537407797681771)\n",
      "(('fact', 'checkers'), 0.00010537407797681771)\n",
      "(('federal', 'income'), 0.00010537407797681771)\n",
      "(('financial', 'disclosure'), 0.00010537407797681771)\n",
      "(('fly', 'lists'), 0.00010537407797681771)\n",
      "(('gold', 'standard'), 0.00010537407797681771)\n",
      "(('good', 'deal'), 0.00010537407797681771)\n",
      "(('good', 'jobs'), 0.00010537407797681771)\n",
      "(('good', 'luck'), 0.00010537407797681771)\n",
      "(('great', 'credit'), 0.00010537407797681771)\n",
      "(('greatest', 'assets'), 0.00010537407797681771)\n",
      "(('greatest', 'threat'), 0.00010537407797681771)\n",
      "(('heard', 'donald'), 0.00010537407797681771)\n",
      "(('hillary', 'clinton'), 0.00010537407797681771)\n",
      "(('important', 'issues'), 0.00010537407797681771)\n",
      "(('income', 'tax'), 0.00010537407797681771)\n",
      "(('iraqi', 'government'), 0.00010537407797681771)\n",
      "(('january', '1st'), 0.00010537407797681771)\n",
      "(('justice', 'department'), 0.00010537407797681771)\n",
      "(('large', 'extent'), 0.00010537407797681771)\n",
      "(('law', 'enforcement'), 0.00010537407797681771)\n",
      "(('lester', 'holt'), 0.00010537407797681771)\n"
     ]
    }
   ],
   "source": [
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 100 frequencies\")\n",
    "for item in scored[:100]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score by PMI and display the top 50 bigrams\n",
    "# only use frequently occurring words in mutual information\n",
    "finder.apply_freq_filter(6)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('sean', 'hannity'), 11.212192371908472),\n",
       " (('middle', 'class'), 10.40483744985087),\n",
       " (('united', 'states'), 10.305301776299956),\n",
       " (('middle', 'east'), 10.234912448408554),\n",
       " (('barack', 'obama'), 10.04226737046616),\n",
       " (('trade', 'deals'), 9.949157966074678),\n",
       " (('tax', 'returns'), 8.81987494912971),\n",
       " (('long', 'time'), 8.653225079720261),\n",
       " (('president', 'obama'), 8.287379868302692),\n",
       " (('bring', 'back'), 7.951664821685252),\n",
       " (('mr.', 'trump'), 6.902864313800746),\n",
       " (('secretary', 'clinton'), 6.821378539720099)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 15 mutual information scores\n",
      "(('sean', 'hannity'), 11.212192371908472)\n",
      "(('middle', 'class'), 10.40483744985087)\n",
      "(('united', 'states'), 10.305301776299956)\n",
      "(('middle', 'east'), 10.234912448408554)\n",
      "(('barack', 'obama'), 10.04226737046616)\n",
      "(('trade', 'deals'), 9.949157966074678)\n",
      "(('tax', 'returns'), 8.81987494912971)\n",
      "(('long', 'time'), 8.653225079720261)\n",
      "(('president', 'obama'), 8.287379868302692)\n",
      "(('bring', 'back'), 7.951664821685252)\n",
      "(('mr.', 'trump'), 6.902864313800746)\n",
      "(('secretary', 'clinton'), 6.821378539720099)\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nBigrams from file with top 15 mutual information scores\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
