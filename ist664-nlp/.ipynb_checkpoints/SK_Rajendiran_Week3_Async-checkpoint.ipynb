{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sathish Kumar Rajendiran\n",
    "Chapter :  Regular Expressions\n",
    "Date: 10/18/2020\n",
    "Week: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sathishrajendiran/ist664-nlp'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# csv, xls, pandas & json\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import xlrd\n",
    "\n",
    "# Language Processing\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# web requests\n",
    "from urllib import request\n",
    "\n",
    "\n",
    "##  Regular Expression to match non-alphabetic characters\n",
    "import re\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sample text\n",
    "text= '''That U.S.A. poster-print from NG costs $12.40, and there are 1,259,000 copies.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a match:\n",
      " ['U.S.A.', 'NG']\n"
     ]
    }
   ],
   "source": [
    "#Phone Numbers\n",
    "if re.search('[A-Z][.]|[A-Z]\\w\\b',text):\n",
    "    text_phone = re.findall(r'\\b(?:[a-zA-Z]\\.){2,}|[A-Z]\\w\\b',text)\n",
    "    print('Found a match:\\n',text_phone )\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "887071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the book Emma from the Gutenberg collection and keep as raw text\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "print(type(emmatext))\n",
    "print(len(emmatext))\n",
    "# display the first 150 characters of the str emmatext\n",
    "emmatext[:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# print the first 150 characters in the str emmatext as one string\n",
    "print(emmatext[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "E\n",
      "m\n",
      "m\n",
      "a\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "J\n",
      "a\n",
      "n\n",
      "e\n",
      " \n",
      "A\n",
      "u\n",
      "s\n",
      "t\n",
      "e\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# print the first 20 characters in emmatext by iterating over the characters\n",
    "for c in emmatext[:20]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Review of strings and string operations\n",
    "# replace end-of-line character with a space\n",
    "newemmatext = emmatext.replace('\\n', ' ')\n",
    "newemmatext[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "### Development of regular expressions for tokenizing text\n",
    "import re\n",
    "# pattern to match words, i.e. anything with a sequence of word characters, ignores special chars\n",
    "shorttext = 'That book is interesting.'\n",
    "pword = re.compile('\\w+')\n",
    "print(re.findall(pword, shorttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "specialtext = 'That U.S.A. poster-print costs $12.40, but with 10% off.'\n",
    "print(re.findall(pword, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', ''), ('U', ''), ('S', ''), ('A', ''), ('poster-print', '-print'), ('costs', ''), ('12', ''), ('40', ''), ('but', ''), ('with', ''), ('10', ''), ('off', '')]\n",
      "[('end-of-line', '-line'), ('character', '')]\n"
     ]
    }
   ],
   "source": [
    "# pattern to match words with internal hyphens\n",
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n",
      "['end-of-line', 'character']\n"
     ]
    }
   ],
   "source": [
    "# ignore the group of the inner parentheses \n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# abbreviations like U.S.A.\n",
    "pabbrev = re.compile('((?:[A-Z]\\.)+)')\n",
    "print(re.findall(pabbrev, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# combine this pattern with the words to make more general tokens\n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*|(?:[A-Z]\\.)+)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# switch the order of the patterns to first match abbreviations and then other words\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# add expression for currency\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*|\\$?\\d+(?:\\.\\d+)?)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'That', ''), ('U.S.A.', '', ''), ('', 'poster-print', ''), ('', 'costs', ''), ('', '', '$12.40'), ('', 'but', ''), ('', 'with', ''), ('', '10', ''), ('', 'off', '')]\n"
     ]
    }
   ],
   "source": [
    "# this is an equivalent regular expression except that it has extra parentheses\n",
    "ptoken = re.compile(r'''((?:[A-Z]\\.)+) # abbreviations, e.g. U.S.A.\n",
    "   | (\\w+(?:-\\w+)*) # words with internal hyphens\n",
    "   | (\\$?\\d+(?:\\.\\d+)?) # currency, like $12.40\n",
    "   ''', re.X) # verbose flag\n",
    "\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More about findall()\n",
    "# using the findall() function to find 2 parts of each match\n",
    "email_text = \"For more information, send a request to info@ischool.syr.edu. Or you can directly contact our information staff at HelpfulHenry@syr.edu and SageSue@syr.edu.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: info, Domain:ischool.syr.edu.\n",
      "User: HelpfulHenry, Domain:syr.edu\n",
      "User: SageSue, Domain:syr.edu.\n"
     ]
    }
   ],
   "source": [
    "# re with two parentheses to match username and domain in every email address\n",
    "pemail = re.compile('([a-zA-Z]+)@([a-z.]+)')\n",
    "matches = re.findall(pemail, email_text)\n",
    "for m in matches:\n",
    "    # format function puts each argument into the output string where the {} is\n",
    "    email = 'User: {}, Domain:{}'.format(m[0],m[1])\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting', '.']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', ',', 'but', 'with', '10%', 'off', '.']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', ',', 'but', 'with', '10', '%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "### using NLTK's regular expression tokenizer\n",
    "# first define a multi-line string that is a regular expression\n",
    "pattern = r''' (?x) \t# set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+    # abbreviations, e.g. U.S.A.\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?    # currency and percentages, $12.40, 50%\n",
    "        | \\w+(?:-\\w+)*  # words with internal hyphens\n",
    "        | \\.\\.\\.        # ellipsis\n",
    "        | [][.,;”’?():-_%#’]    # separate tokens\n",
    "        '''\n",
    "\n",
    "# the nltk regular expression tokenizer compiles the re pattern, applies it to the text\n",
    "#  and uses the matching groups to return a list of only the matched tokens\n",
    "print(nltk.regexp_tokenize(shorttext, pattern))\n",
    "print(nltk.regexp_tokenize(specialtext, pattern))\n",
    "\n",
    "# compare with built-in word tokenizer\n",
    "print(nltk.word_tokenize(specialtext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for Twitter derived tweetmotif from the ARK, developed at CMU\n",
    "tweetPattern = r''' (?x)\t# set flag to allow verbose regexps\n",
    "      (?:https?://|www)\\S+      # simple URLs\n",
    "      | (?::-\\)|;-\\))\t\t# small list of emoticons\n",
    "      | &(?:amp|lt|gt|quot);    # XML or HTML entity\n",
    "      | \\#\\w+                 # hashtags\n",
    "      | @\\w+                  # mentions   \n",
    "      | \\d+:\\d+               # timelike pattern\n",
    "      | \\d+\\.\\d+              # number with a decimal\n",
    "      | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$))   # number with a comma\n",
    "      | (?:[A-Z]\\.)+                    # simple abbreviations\n",
    "      | (?:--+)               # multiple dashes\n",
    "      | \\w+(?:-\\w+)*          # words with internal hyphens or apostrophes\n",
    "      | ['\\\".?!,:;/]+         # special characters\n",
    "      '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare,  I trust him. #p2 #tlot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', 'GOV', \"'\", 'T', 'TAKEOVER', '!']\n",
      "['To', 'Sen', '.', 'Roland', 'Burris', ':', 'Affordable', ',', 'quality', 'health', 'insurance', 'can', \"'\", 't', 'wait', 'http://bit.ly/j63je', '#hc09', '#IL', '#60660']\n",
      "['RT', '@karoli', ':', 'RT', '@Seriou', ':', '.', '@whitehouse', 'I', 'will', 'stand', 'w', '/', 'Obama', 'on', '#healthcare', ',', 'I', 'trust', 'him', '.', '#p2', '#tlot']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(tweet1,tweetPattern))\n",
    "print(nltk.regexp_tokenize(tweet2,tweetPattern))\n",
    "print(nltk.regexp_tokenize(tweet3,tweetPattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', '-', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', \"GOV'T\", 'TAKEOVER', '!']\n"
     ]
    }
   ],
   "source": [
    "# NLTK built-in tokenizer (more detailed version from TweetMotif)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ttokenizer = TweetTokenizer()\n",
    "print(ttokenizer.tokenize(tweet1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', '.', 'Black', 'and', 'Mrs', '.', 'Brown', 'attended', 'the', 'lecture', 'by', 'Dr', '.', 'Gray', ',', 'but', 'Gov', '.', 'White', 'wasn', 't', 'there', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence example for the question\n",
    "\n",
    "sent = \"Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn't there.\"\n",
    "print(nltk.regexp_tokenize(sent, pattern))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
